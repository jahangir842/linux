# Apache Spark (Processing Engine)
## How to Use Apache Spark

### 1. Setup and Installation
To start using Apache Spark, you need to set up a Spark cluster or run it locally on your machine. Here are the steps for a local setup:

#### Install Java
For Ubuntu:
```sh
sudo apt install default-jdk
```
For RHEL:
```sh
sudo yum install java-devel
```

#### Download and Install Apache Spark
```sh
wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
tar -xvzf spark-3.1.2-bin-hadoop3.2.tgz
sudo mv spark-3.1.2-bin-hadoop3.2 /opt/spark
sudo chmod -R 777 /opt/spark
```

#### Update `bashrc`
```sh
echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin:$JAVA_HOME/bin" >> ~/.bashrc
source ~/.bashrc
```

### 2. Configure Spark
Edit the `conf/spark-env.sh` file on the master node. If it doesnâ€™t exist, create it from the template:
```sh
cp /opt/spark/conf/spark-env.sh.template /opt/spark/conf/spark-env.sh
nano /opt/spark/conf/spark-env.sh
```
Add the following configuration:
```sh
export SPARK_MASTER_HOST='master-node-ip'
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

Create a `workers` file in the `conf` directory on the master node and add the IP addresses of your worker nodes:
```sh
cp /opt/spark/conf/worker.template /opt/spark/conf/worker
echo 'worker-node-1-ip' >> /opt/spark/conf/workers
echo 'worker-node-2-ip' >> /opt/spark/conf/workers
echo 'worker-node-3-ip' >> /opt/spark/conf/workers
```

### 3. Start Spark in Terminal
- Scala: `spark-shell`
- Python: `pyspark`

To view the Spark Web UI:  
`http://<IP_ADDRESS>:4040`

### 4. Start Standalone Spark Master Server
In the terminal, type:
```sh
start-master.sh
```
Access the web UI at `http://<IP_ADDRESS>:8080`

### Additional Installation: Python
#### Install Python 3 Pip
If pip (Python package installer) is not already installed, you can install it using:
```sh
sudo apt install python3-pip
```

#### Install PySpark with Pip
```sh
pip3 install pyspark
```

#### Verify Installation
```sh
pyspark
```

### Summary of Installations
- **Core Spark Installation**: The tar file provides the core Spark installation, including all necessary binaries and configuration files needed to run Spark.
- **Python Integration**: Installing PySpark with `pip3` sets up the Python bindings and dependencies, making it possible to use Spark with Python and ensuring compatibility with other Python packages.

This dual approach ensures that your Spark installation is complete and that you can effectively use Spark within your Python environment.

### Generate SSH Key Pair on Master Node and Send to All Workers
Passwordless SSH access is needed on the master node to allow seamless communication and task distribution between the master and worker nodes in a Spark cluster. It enables the master node to remotely execute commands and manage the worker nodes without manual password entry, facilitating automated cluster operations and job execution.

#### Generate SSH Key Pair
```sh
ssh-keygen -t rsa -b 4096 -C "your_email@example.com"
```

Ensure the SSH agent is running (optional but recommended for managing keys):
```sh
eval "$(ssh-agent -s)"
```

Add your private key to the SSH agent:
```sh
ssh-add ~/.ssh/id_rsa
```

#### Copy the Public Key to Worker Nodes
```sh
ssh-copy-id user@worker-node-1-ip
ssh-copy-id user@worker-node-2-ip
ssh-copy-id user@worker-node-3-ip
```
Replace `user` with your actual username on the worker nodes.

You should be able to log in to each worker node without being prompted for a password.
