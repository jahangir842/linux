****************************************************************
        		Apache Spark (a processing engine)
****************************************************************
Apache Spark is an open-source, distributed computing system designed for fast processing of large-scale data. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Here’s an overview of its key components and capabilities:
****************************************************************
Key Components of Apache Spark:
****************************************************************
1. Spark Core: The foundation of Spark, providing basic I/O functionalities, task scheduling, and memory management.
2. Spark SQL: Module for structured data processing. It provides a programming interface for working with structured data using SQL.
3. Spark Streaming: Enables processing of live data streams.
4. MLlib: Spark’s machine learning library.
5. GraphX: API for graphs and graph-parallel computation.
****************************************************************
How to Use Apache Spark:
****************************************************************
#1. Setup and Installation:
To start using Apache Spark, you need to set up a Spark cluster or run it locally on your machine. Here are the steps for a local setup:
****************************************************************
Install Java:
****************************************************************
for Ubuntu:
sudo apt install default-jdk
for RHEL:
sudo yum install java-devel

****************************************************************
Download and Install Apache Spark:
****************************************************************
  wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
  tar -xvzf spark-3.1.2-bin-hadoop3.2.tgz
 
  sudo mv spark-3.2.0-bin-hadoop3.2 /opt/spark
  sudo chmod -R 777 /opt/spark

****************************************************************  
Update bashrc: 
****************************************************************
echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> ~/.bashrc
source ~/.bashrc
****************************************************************
Start the Spark in Terminal:
****************************************************************
     - Scala: `spark-shell`
     - Python: `pyspark`
     
To view the Spark Web user interface:

http://<IP ADDRESS>:4040

The page shows your Spark URL, status information for workers, hardware resource utilization, etc.   ****************************************************************    
Start Standalone Spark Master Server
****************************************************************
In the terminal, type:

start-master.sh
start-worker.sh
http://<IP ADDRESS>:8080
****************************************************************
Additionaly Install Python:
****************************************************************
Install Python 3 Pip: If pip (Python package installer) is not already installed, you can install it using 
	sudo apt install python3-pip

Install PySpark with Pip:
	pip3 install pyspark

Verify Installation:
	pyspark
****************************************************************
By following above steps: We did 2 type of installations:
****************************************************************
    Core Spark Installation: The tar file provides the core Spark installation, including all necessary binaries and configuration files needed to run Spark.

    Python Integration: Installing PySpark with pip3 sets up the Python bindings and dependencies, making it possible to use Spark with Python and ensuring compatibility with other Python packages.

This dual approach ensures that your Spark installation is complete and that you can effectively use Spark within your Python environment.
****************************************************************
