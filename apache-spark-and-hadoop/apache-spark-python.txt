****************************************************************
Key Terminologies related to Apache Spark:
****************************************************************
1. SparkContext:
    - The main entry point for Spark functionality. It is responsible for coordinating the execution of tasks over a cluster of machines and managing resources. SparkContext is used to create RDDs and connect to Spark cluster managers.
****************************************************************
2. RDD (Resilient Distributed Dataset):
    - The fundamental data structure of Spark, representing a distributed collection of elements that can be processed in parallel. RDDs are immutable and fault-tolerant, meaning they can be recomputed if lost.
****************************************************************
3. Transformation:
    - Operations that create a new RDD from an existing one. Transformations are lazy, meaning they are not executed immediately but are instead recorded as a lineage graph to be executed when an action is called. Examples include `map()`, `flatMap()`, `filter()`, and `reduceByKey()`.
****************************************************************
4. Action:
    - Operations that trigger the execution of transformations to return a result to the driver program or write data to an external storage system. Actions include `collect()`, `count()`, `saveAsTextFile()`, and `take()`.
****************************************************************
5. DAG (Directed Acyclic Graph):
    - A graphical representation of the sequence of transformations on RDDs that Spark constructs to track the lineage of computations. The DAG ensures the immutability of RDDs and fault tolerance.
****************************************************************
6. Partition:
    - A logical division of an RDD, representing a chunk of data that can be processed independently. Partitions enable parallel processing in Spark, as each partition can be processed on a different node in the cluster.
****************************************************************
7. Cluster Manager:
    - A system that manages resources (CPU, memory) and schedules tasks on a cluster of machines. Examples of cluster managers that Spark can work with include YARN, Mesos, and the standalone cluster manager.
****************************************************************
8. Executor:
    - A distributed agent responsible for executing tasks on worker nodes in the cluster. Each executor runs tasks in its own process and stores data for RDDs that are cached.
****************************************************************
9. Driver Program:
    - The main program that runs on the client machine and orchestrates the execution of Spark jobs. It creates the SparkContext and submits jobs to the cluster.
****************************************************************
10. Task:
    - A unit of work sent to an executor. Each task processes a partition of an RDD. Tasks are the smallest units of work in Spark.
****************************************************************
11. Job:
    - A sequence of transformations triggered by an action. Each job is divided into stages and tasks. A job corresponds to a single action call in the driver program.
****************************************************************
12. Stage:
    - A group of tasks that can be executed in parallel. Stages are determined by shuffle operations, which require data to be redistributed across partitions.
****************************************************************
13. Shuffle:
    - The process of redistributing data across partitions to perform operations such as `reduceByKey` or `groupByKey`. Shuffling involves network and disk I/O, making it one of the more expensive operations in Spark.
****************************************************************
14. Lazy Evaluation:
    - A feature of Spark where transformations are not executed immediately but are recorded in a lineage graph. The actual execution of transformations is deferred until an action is called, optimizing the execution plan.
****************************************************************
15. Broadcast Variable:
    - A read-only variable cached on each machine rather than shipping a copy of it with tasks. Broadcast variables are used to efficiently distribute large values (like lookup tables) to executors.
****************************************************************
16. Accumulator:
    - A variable that is used for aggregating information across tasks. Accumulators are used for implementing counters and sums. Only the driver program can read the value of the accumulator, but tasks can add to it.
****************************************************************

Example Usage of Some Terms: (for explaination)

****************************************************************
#:Example: Using RDDs, Transformations, Actions, and SparkContext
****************************************************************
from pyspark import SparkContext

# Initialize SparkContext
sc = SparkContext(master="local", appName="ExampleApp")

# Local collection
data = [1, 2, 3, 4, 5]

# Create an RDD
rdd = sc.parallelize(data)

# Perform a transformation (map) and an action (collect)
squared_rdd = rdd.map(lambda x: x * x)
result = squared_rdd.collect()

print(result)  # Output: [1, 4, 9, 16, 25]

# Stop the SparkContext
sc.stop()
****************************************************************

- **SparkContext: `sc` is the SparkContext, the main entry point for Spark.
- **RDD: `rdd` is an RDD created from the local collection `data`.
- **Transformation: `map` is a transformation applied to `rdd`.
- **Action: `collect` is an action that triggers the execution of the transformation and collects the results.

Understanding these key terminologies is essential for effectively working with Apache Spark and building efficient data processing pipelines.

****************************************************************
Writing and Running Spark Applications on cluster
****************************************************************
Step 1 Write the PySpark script (example.py):
****************************************************************
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .appName("ExampleApp") \
    .config("spark.master", "spark://master-node-ip:7077") \
    .getOrCreate()

# Define your task or job
data = [('Alice', 1), ('Bob', 2), ('Cathy', 3)]
df = spark.createDataFrame(data, ['Name', 'Age'])
df.show()

# Stop the SparkSession
spark.stop()


****************************************************************
Step 2 Run the script:
****************************************************************
  spark-submit --master spark://master-node-ip:7077 --num-executors 2 --executor-cores 1 example.py
****************************************************************



****************************************************************
What is SparkContext:
****************************************************************
The SparkContext is the main entry point for any Spark functionality and is responsible for coordinating the execution of tasks over a cluster of machines. 
****************************************************************
Key Functions of SparkContext:

Connecting to a Cluster:
SparkContext allows you to connect to a Spark cluster, which can be a standalone cluster, Mesos, YARN, or Kubernetes.

Creating RDDs:
RDDs are the fundamental data structure in Spark. SparkContext provides methods to create RDDs from various data sources like local files, HDFS, HBase, Cassandra, and more.

Configuration:
You can configure various parameters like the number of cores, memory allocation, and other settings when initializing SparkContext.

Managing Jobs:
SparkContext is responsible for managing the job execution, including task scheduling and fault recovery.

****************************************************************
How to Use SparkContext
****************************************************************
Here's an example of python script:
****************************************************************
from pyspark import SparkContext, SparkConf

# Configure Spark
conf = SparkConf().setAppName("MyClusterApp").setMaster("spark://master:7077")
sc = SparkContext(conf=conf)

# Create an RDD from a file on HDFS
rdd = sc.textFile("hdfs://master:9000/user/hadoop/input.txt")

# Perform transformations and actions
result = rdd.flatMap(lambda line: line.split()).countByValue()

print(result)

# Stop the SparkContext
sc.stop()

****************************************************************
Key Methods of SparkContext

parallelize(collection): Distributes a local Python collection to form an RDD.
textFile(path): Reads a text file from HDFS, a local file system, or any Hadoop-supported file system URI.
setLogLevel(logLevel): Sets the log level (e.g., "INFO", "DEBUG", "ERROR").
stop(): Stops the SparkContext, which terminates the application.

