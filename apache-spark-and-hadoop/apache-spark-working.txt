****************************************************************
Start the Spark in Terminal:
     - Scala: `spark-shell`
     - Python: `pyspark`
****************************************************************
To view the Spark Web user interface, open a web browser and enter the localhost IP address on port 4040.

http://<IP ADDRESS>:4040

The page shows your Spark URL, status information for workers, hardware resource utilization, etc.   ****************************************************************    
Start Standalone Spark Master Server
****************************************************************
Basic Commands to Start and Stop Master Server and Workers

start-master.sh
start-worker.sh
http://<IP ADDRESS>:8080
****************************************************************
#2. Writing and Running Spark Applications:
****************************************************************
Spark applications can be written in Scala, Python, Java, and R. Below are examples in Python (using PySpark).
****************************************************************
**Example: Word Count Application**
****************************************************************
Create a text file with following command:
****************************************************************
	touch textfile.txt
	echo 'this is a test file for apache spark' > textfile.txt
****************************************************************
Step 1 Write the PySpark script (word_count.py):
****************************************************************
  from pyspark import SparkContext

  if __name__ == "__main__":
      sc = SparkContext("local", "Word Count")
      text_file = sc.textFile("./textfile.txt")
      counts = text_file.flatMap(lambda line: line.split(" ")) \
                        .map(lambda word: (word, 1)) \
                        .reduceByKey(lambda a, b: a + b)
      counts.saveAsTextFile("output")

****************************************************************
Step 2 Run the script:
****************************************************************
  spark-submit word_count.py
****************************************************************
Apache Spark can run in two primary deployment modes: client mode and cluster mode. These modes determine where the Spark Driver and Spark Executors are located and how they communicate. Understanding the differences between these modes is crucial for effectively deploying and managing Spark applications.

### Client Mode

In client mode, the Spark Driver runs on the machine from which the Spark application is submitted (often a user's local machine or an edge node in a cluster). The Spark Executors run on the worker nodes in the cluster.

**Key Characteristics of Client Mode:**
1. **Driver Location:** The driver runs on the client machine (the machine where the job is submitted).
2. **Network Communication:** The driver must communicate with the executors, requiring network connectivity between the client machine and the worker nodes.
3. **Use Case:** Suitable for interactive and development purposes where the user needs quick feedback and is typically running the application from their local machine.
4. **Resource Management:** The client machine must have enough resources (CPU, memory) to run the driver process, as the driver is responsible for maintaining information about the applicationâ€™s structure and scheduling tasks.

**Example:**
```shell
spark-submit --master yarn --deploy-mode client my_spark_app.py
```

### Cluster Mode

In cluster mode, the Spark Driver runs inside the cluster. When the application is submitted, the cluster manager (e.g., YARN, Kubernetes, Mesos) launches the driver inside the cluster.

**Key Characteristics of Cluster Mode:**
1. **Driver Location:** The driver runs on a worker node inside the cluster, not on the client machine.
2. **Network Communication:** Since the driver runs in the cluster, it has a better network proximity to the executors, which can reduce communication latency.
3. **Use Case:** Suitable for production jobs and batch processing where the application needs to run reliably without user interaction and the client machine does not need to remain connected.
4. **Resource Management:** The cluster handles the resources for both the driver and the executors, ensuring better resource utilization and management.

**Example:**
```shell
spark-submit --master yarn --deploy-mode cluster my_spark_app.py
```

### Key Differences

1. **Driver Location:**
   - **Client Mode:** Runs on the machine from which the job is submitted.
   - **Cluster Mode:** Runs on a worker node within the cluster.

2. **Network Requirements:**
   - **Client Mode:** Requires network connectivity between the client machine and the cluster.
   - **Cluster Mode:** Network communication is internal to the cluster.

3. **Use Cases:**
   - **Client Mode:** Ideal for interactive and development purposes.
   - **Cluster Mode:** Ideal for production and long-running batch jobs.

4. **Resource Management:**
   - **Client Mode:** Client machine must be resourceful enough to handle the driver process.
   - **Cluster Mode:** Cluster resources are used to run both driver and executors.

### Conclusion

Choosing between client mode and cluster mode depends on the specific requirements of your Spark application. For development and interactive use cases, client mode offers a convenient way to run and test applications. For production environments, where stability and resource management are crucial, cluster mode is typically preferred.

****************************************************************
Apache Spark Tools and Commands:
****************************************************************
1. spark-submit
****************************************************************
   Purpose Used to submit Spark applications to a cluster for execution.
****************************************************************   
   Syntax
****************************************************************
     spark-submit \
       --class <main-class> \
       --master <master-url> \
       --deploy-mode <deploy-mode> \
       --executor-memory <memory-per-executor> \
       --total-executor-cores <num-cores> \
       <application-jar> [application-arguments]
****************************************************************
   Example
****************************************************************
     spark-submit \
       --class com.example.MySparkApp \
       --master spark://localhost:7077 \
       --deploy-mode client \
       --executor-memory 2g \
       --total-executor-cores 4 \
****************************************************************
2. Spark on Terminal: 
****************************************************************
(spark-shell for Scala, pyspark for Python)
****************************************************************
   Purpose Interactive shell for running Spark code snippets and testing. The Spark Shell is an interactive environment provided by Apache Spark that allows you to quickly prototype, test, and execute Spark commands in a REPL (Read-Eval-Print Loop) environment. It is especially useful for data exploration and iterative development, as it provides immediate feedback on your code.
****************************************************************   
   Usage
     - Scala: `spark-shell`
     - Python: `pyspark`
****************************************************************
To start the PySpark shell, open your terminal and type:
	pyspark
****************************************************************
This command initializes the PySpark environment and opens an interactive shell where you can start writing Spark commands in Python.
****************************************************************
it automatically creates a SparkSession and a SparkContext for you:
****************************************************************
Try the following Python code
****************************************************************
from pyspark.sql import SparkSession

# SparkSession automatically created as 'spark'
print(spark)

# SparkContext available as 'sc'
print(sc)
****************************************************************
Creating RDDs:
****************************************************************
You can create Resilient Distributed Datasets (RDDs) from a Python collection or an external data source. use this code:
****************************************************************

# Create an RDD from a Python list
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Perform a transformation (map) and action (collect)
squares = rdd.map(lambda x: x * x)
print(squares.collect())
****************************************************************
Working with DataFrames:
****************************************************************
DataFrames provide a higher-level abstraction over RDDs and are optimized for performance. use this code:
****************************************************************
# Creating a DataFrame from a JSON file
df = spark.read.json("path/to/json/file")

# Show the first few rows of the DataFrame
df.show()

# Print the schema of the DataFrame
df.printSchema()
****************************************************************



    
****************************************************************
6. Spark Web UI
****************************************************************
   Purpose Web-based interface for monitoring Spark applications and cluster resources.
   Access Typically available at `http://<driver-node>:4040`.
****************************************************************
7. Spark History Server
****************************************************************
   Purpose Retains information about completed Spark applications and allows viewing historical data and logs.
   Access Typically available at `http://<history-server>:18080`.
****************************************************************
Cluster Managers 
****************************************************************
Spark can run on various cluster managers like Spark's standalone cluster manager, Apache Hadoop YARN, or Apache Mesos. The `--master` parameter in `spark-submit` specifies the cluster manager.
****************************************************************
Resource Allocation Use `--executor-memory` and `--total-executor-cores` in `spark-submit` to allocate resources to Spark executors.
****************************************************************
Environment Setup Set `SPARK_HOME`, update `PATH`, and configure other environment variables for seamless Spark usage.
****************************************************************
Integration Spark integrates with other big data tools and ecosystems, such as Apache Hadoop, Apache Hive, Apache HBase, etc.
****************************************************************
Understanding and using these tools and commands efficiently will help in developing, deploying, and managing Apache Spark applications effectively in various use cases and environments.

****************************************************************
Using Spark on AWS:
****************************************************************
For large-scale applications, you might want to run Spark on a cluster. AWS provides several ways to run Spark:
****************************************************************
Amazon EMR (Elastic MapReduce) Managed cluster platform that simplifies running big data frameworks such as Apache Spark.
****************************************************************
  aws emr create-cluster --name "Spark cluster" \
    --release-label emr-6.2.0 \
    --applications Name=Spark \
    --ec2-attributes KeyName=myKey \
    --instance-type m5.xlarge \
    --instance-count 3
****************************************************************
Amazon EKS (Elastic Kubernetes Service) You can run Spark on Kubernetes clusters.
AWS Glue Fully managed ETL service that can run Spark jobs.
****************************************************************
Apache Spark and Apache Hadoop
****************************************************************
While Spark originally started as a project in the Hadoop ecosystem and can integrate well with Hadoop's distributed file system (HDFS) and resource manager (YARN), it does not have a strict dependency on Hadoop. Here are a few points to consider:
****************************************************************
    Stand-alone Mode: Spark can be run in stand-alone mode, where it uses its own built-in cluster manager. In this mode, Spark does not require Hadoop's components such as HDFS or YARN. It manages its own resources and scheduling.
****************************************************************
    Local Mode: For development and testing purposes, Spark can also run in local mode on a single machine without the need for a Hadoop cluster. This is particularly useful for getting started with Spark, experimenting with code, and running small-scale jobs.
****************************************************************
    Compatibility with Other Storage Systems: While Spark can work seamlessly with Hadoop's HDFS, it's also compatible with other storage systems such as Amazon S3, Azure Blob Storage, Google Cloud Storage, and various databases (e.g., Apache Cassandra, Apache HBase) through connectors and libraries.
****************************************************************
    Resource Managers: Spark supports multiple cluster managers apart from Hadoop's YARN, such as Apache Mesos and its stand-alone cluster manager. This flexibility allows users to choose the cluster management system that best fits their needs and infrastructure.
****************************************************************
    Dependencies: When you download and install Apache Spark, you'll notice that it includes all the necessary libraries and components to run Spark applications independently. This includes Spark Core, Spark SQL, MLlib, Spark Streaming, and GraphX, among others.
****************************************************************
So, if your use case doesn't require Hadoop-specific functionalities or if you're just getting started with Spark, you can install and use Spark without setting up Hadoop. However, if you're dealing with large-scale distributed data storage (e.g., HDFS) or need integration with Hadoop's ecosystem components, then using Spark with Hadoop would be appropriate.
****************************************************************
