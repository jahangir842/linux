*****************************************************************
list of commands for PySpark
*****************************************************************
------------------------------------------------------------------------------------------------------
| Command                               | Function                                                   |
|---------------------------------------|------------------------------------------------------------|
| `pyspark`                             | Starts the PySpark shell, allowing interactive Spark usage.|
| `spark-submit <Python/Spark script>`  | Submits a Spark application for execution.                 |
| `spark-shell`                         | Starts the Spark shell for Scala-based Spark development.   |
| `spark-sql`                           | Starts the Spark SQL CLI for executing SQL commands.       |
| `spark-class <class>`                 | Runs a Java/Scala class within the Spark environment.      |
| `spark-submit --master <master_url> ...` | Submits a Spark application to a specific master URL.     |
| `spark-submit --packages <package_name>` | Submits a Spark application with specified packages.      |
| `spark-submit --jars <jar_file>`      | Submits a Spark application with additional JAR files.     |
| `spark-submit --help`                 | Displays help information for using `spark-submit`.         |

****************************************************************
Start the Spark in Terminal:
****************************************************************
     - Scala: `spark-shell`
     - Python: `pyspark`
     
To view the Spark Web user interface:

http://<IP ADDRESS>:4040

The page shows your Spark URL, status information for workers, hardware resource utilization, etc.   ****************************************************************    
Start the Spark Master Server in Standalone setup:
****************************************************************
In the terminal, type:
	start-master.sh

open a web browser and enter the localhost IP address on port 8080.
	http://<IP ADDRESS>:8080

The page will show your Spark URL, status information for workers, hardware resource utilization, etc.
****************************************************************
Start a Worker Process in Standalone setup:
****************************************************************
In the terminal, type:

	start-worker.sh spark://<master-ip>:7077

The master in the command can be an IP or hostname.

Now that a worker is up and running, if you reload Spark Master’s Web UI, you should see it on the list:
****************************************************************
Specify Resource Allocation (Cores) for Workers
****************************************************************
The default setting when starting a worker on a machine is to use all available CPU cores. You can specify the number of cores by passing the -c flag to the start-slave command.

For example, to start a worker and assign only one CPU core to it, enter this command:

	start-worker.sh -c 1 spark://<master-ip>:port

Reload Spark Master’s Web UI to confirm the worker’s configuration.

****************************************************************

Similarly, you can assign a specific amount of memory when starting a worker. The default setting is to use whatever amount of RAM your machine has, minus 1GB.

To start a worker and assign it a specific amount of memory, add the -m option and a number. For gigabytes, use G and for megabytes, use M.

For example, to start a worker with 512MB of memory, enter this command:

	start-worker.sh -m 512M spark://<master-ip>:port
	start-worker.sh -c 2 -m 1G spark://<master-ip>:7077

Reload the Spark Master Web UI to view the worker’s status and confirm the configuration


****************************************************************
To stop the master instance started by executing the script above, run:

stop-master.sh

To stop a running worker process, enter this command:

stop-worker.sh

****************************************************************
You can start both master and server instances by using the start-all command:

start-all.sh

Similarly, you can stop all instances by using the following command:

stop-all.sh


****************************************************************
Apache Spark can run in two primary deployment modes: client mode and cluster mode. ****************************************************************
### Client Mode
****************************************************************
In client mode, the Spark Driver runs on the machine from which the Spark application is submitted (often a user's local machine or an edge node in a cluster). The Spark Executors run on the worker nodes in the cluster.

Key Characteristics of Client Mode:
1. Driver Location: The driver runs on the client machine (the machine where the job is submitted).

2. Network Communication: The driver must communicate with the executors, requiring network connectivity between the client machine and the worker nodes.

3. Use Case: Suitable for interactive and development purposes where the user needs quick feedback and is typically running the application from their local machine.

4. Resource Management: The client machine must have enough resources (CPU, memory) to run the driver process, as the driver is responsible for maintaining information about the application’s structure and scheduling tasks.

Example:

      spark-submit --master yarn --deploy-mode client my_spark_app.py

****************************************************************
### Cluster Mode
****************************************************************
In cluster mode, the Spark Driver runs inside the cluster. When the application is submitted, the cluster manager (e.g., YARN, Kubernetes, Mesos) launches the driver inside the cluster.
****************************************************************
**Key Characteristics of Cluster Mode:**
1. **Driver Location:** The driver runs on a worker node inside the cluster, not on the client machine.
2. **Network Communication:** Since the driver runs in the cluster, it has a better network proximity to the executors, which can reduce communication latency.
3. **Use Case:** Suitable for production jobs and batch processing where the application needs to run reliably without user interaction and the client machine does not need to remain connected.
4. **Resource Management:** The cluster handles the resources for both the driver and the executors, ensuring better resource utilization and management.
****************************************************************
Example:

spark-submit --master yarn --deploy-mode cluster my_spark_app.py

****************************************************************
### Key Differences

1. **Driver Location:**
   - **Client Mode:** Runs on the machine from which the job is submitted.
   - **Cluster Mode:** Runs on a worker node within the cluster.

2. **Network Requirements:**
   - **Client Mode:** Requires network connectivity between the client machine and the cluster.
   - **Cluster Mode:** Network communication is internal to the cluster.

3. **Use Cases:**
   - **Client Mode:** Ideal for interactive and development purposes.
   - **Cluster Mode:** Ideal for production and long-running batch jobs.

4. **Resource Management:**
   - **Client Mode:** Client machine must be resourceful enough to handle the driver process.
   - **Cluster Mode:** Cluster resources are used to run both driver and executors.
****************************************************************
### Conclusion

Choosing between client mode and cluster mode depends on the specific requirements of your Spark application. For development and interactive use cases, client mode offers a convenient way to run and test applications. For production environments, where stability and resource management are crucial, cluster mode is typically preferred.

****************************************************************
Apache Spark Tools and Commands:
****************************************************************
1. spark-submit
****************************************************************
   Purpose Used to submit Spark applications to a cluster for execution.
****************************************************************   
   Syntax Example
****************************************************************
     spark-submit \
       --class com.example.MySparkApp \
       --master spark://localhost:7077 \
       --deploy-mode client \
       --executor-memory 2g \
       --total-executor-cores 4 \
****************************************************************
2. Spark on Terminal: 
****************************************************************
(spark-shell for Scala, pyspark for Python)
****************************************************************
   Purpose Interactive shell for running Spark code snippets and testing. The Spark Shell is an interactive environment provided by Apache Spark that allows you to quickly prototype, test, and execute Spark commands in a REPL (Read-Eval-Print Loop) environment. It is especially useful for data exploration and iterative development, as it provides immediate feedback on your code.
****************************************************************   
   Usage
     - Scala: `spark-shell`
     - Python: `pyspark`
****************************************************************
To start the PySpark shell, open your terminal and type:
	pyspark
****************************************************************
This command initializes the PySpark environment and opens an interactive shell where you can start writing Spark commands in Python.
****************************************************************
it automatically creates a SparkSession and a SparkContext for you:
****************************************************************
7. Spark History Server
****************************************************************
   Purpose Retains information about completed Spark applications and allows viewing historical data and logs.
   Access Typically available at `http://<history-server>:18080`.
****************************************************************
Cluster Managers 
****************************************************************
Spark can run on various cluster managers like Spark's standalone cluster manager, Apache Hadoop YARN, or Apache Mesos. The `--master` parameter in `spark-submit` specifies the cluster manager.
****************************************************************
Resource Allocation Use `--executor-memory` and `--total-executor-cores` in `spark-submit` to allocate resources to Spark executors.
****************************************************************
Environment Setup Set `SPARK_HOME`, update `PATH`, and configure other environment variables for seamless Spark usage.
****************************************************************
Integration Spark integrates with other big data tools and ecosystems, such as Apache Hadoop, Apache Hive, Apache HBase, etc.
****************************************************************
Understanding and using these tools and commands efficiently will help in developing, deploying, and managing Apache Spark applications effectively in various use cases and environments.

****************************************************************
Using Spark on AWS:
****************************************************************
For large-scale applications, you might want to run Spark on a cluster. AWS provides several ways to run Spark:
****************************************************************
Amazon EMR (Elastic MapReduce) Managed cluster platform that simplifies running big data frameworks such as Apache Spark.
****************************************************************
  aws emr create-cluster --name "Spark cluster" \
    --release-label emr-6.2.0 \
    --applications Name=Spark \
    --ec2-attributes KeyName=myKey \
    --instance-type m5.xlarge \
    --instance-count 3
****************************************************************
Amazon EKS (Elastic Kubernetes Service) You can run Spark on Kubernetes clusters.
AWS Glue Fully managed ETL service that can run Spark jobs.
****************************************************************
Apache Spark and Apache Hadoop
****************************************************************
While Spark originally started as a project in the Hadoop ecosystem and can integrate well with Hadoop's distributed file system (HDFS) and resource manager (YARN), it does not have a strict dependency on Hadoop. Here are a few points to consider:
****************************************************************
    Stand-alone Mode: Spark can be run in stand-alone mode, where it uses its own built-in cluster manager. In this mode, Spark does not require Hadoop's components such as HDFS or YARN. It manages its own resources and scheduling.
****************************************************************
    Local Mode: For development and testing purposes, Spark can also run in local mode on a single machine without the need for a Hadoop cluster. This is particularly useful for getting started with Spark, experimenting with code, and running small-scale jobs.
****************************************************************
    Compatibility with Other Storage Systems: While Spark can work seamlessly with Hadoop's HDFS, it's also compatible with other storage systems such as Amazon S3, Azure Blob Storage, Google Cloud Storage, and various databases (e.g., Apache Cassandra, Apache HBase) through connectors and libraries.
****************************************************************
    Resource Managers: Spark supports multiple cluster managers apart from Hadoop's YARN, such as Apache Mesos and its stand-alone cluster manager. This flexibility allows users to choose the cluster management system that best fits their needs and infrastructure.
****************************************************************
    Dependencies: When you download and install Apache Spark, you'll notice that it includes all the necessary libraries and components to run Spark applications independently. This includes Spark Core, Spark SQL, MLlib, Spark Streaming, and GraphX, among others.
****************************************************************
So, if your use case doesn't require Hadoop-specific functionalities or if you're just getting started with Spark, you can install and use Spark without setting up Hadoop. However, if you're dealing with large-scale distributed data storage (e.g., HDFS) or need integration with Hadoop's ecosystem components, then using Spark with Hadoop would be appropriate.
****************************************************************
