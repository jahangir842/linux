****************************************************************
        		Apache Spark (a processing engine)
****************************************************************
Apache Spark is an open-source, distributed computing system designed for fast processing of large-scale data. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Here’s an overview of its key components and capabilities:
****************************************************************
Key Components of Apache Spark:
****************************************************************
1. Spark Core: The foundation of Spark, providing basic I/O functionalities, task scheduling, and memory management.
2. Spark SQL: Module for structured data processing. It provides a programming interface for working with structured data using SQL.
3. Spark Streaming: Enables processing of live data streams.
4. MLlib: Spark’s machine learning library.
5. GraphX: API for graphs and graph-parallel computation.
****************************************************************
How to Use Apache Spark:
****************************************************************
#1. Setup and Installation:
To start using Apache Spark, you need to set up a Spark cluster or run it locally on your machine. Here are the steps for a local setup:
****************************************************************
Download and Install Apache Spark:
****************************************************************
  wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
  tar -xvzf spark-3.1.2-bin-hadoop3.2.tgz
  cd spark-3.1.2-bin-hadoop3.2

****************************************************************
Set environment variables:
****************************************************************
  Add the following lines to your `.bashrc` or `.zshrc` file:
****************************************************************
  export SPARK_HOME=/path/to/spark-3.1.2-bin-hadoop3.2
  export PATH=$SPARK_HOME/bin:$PATH
****************************************************************
Step 3 Start the Spark in Terminal:
     - Scala: `spark-shell`
     - Python: `pyspark`
****************************************************************
#2. Writing and Running Spark Applications:
****************************************************************
Spark applications can be written in Scala, Python, Java, and R. Below are examples in Python (using PySpark).
****************************************************************
**Example: Word Count Application**
Step 1 Write the PySpark script (word_count.py):
****************************************************************
  from pyspark import SparkContext

  if __name__ == "__main__":
      sc = SparkContext("local", "Word Count")
      text_file = sc.textFile("path/to/textfile.txt")
      counts = text_file.flatMap(lambda line: line.split(" ")) \
                        .map(lambda word: (word, 1)) \
                        .reduceByKey(lambda a, b: a + b)
      counts.saveAsTextFile("output")

****************************************************************
Step 2 Run the script:
****************************************************************
  spark-submit word_count.py
****************************************************************
**Example: Using Spark SQL**
****************************************************************
Step 1 Write the PySpark script (spark_sql_example.py):
****************************************************************
  from pyspark.sql import SparkSession

  spark = SparkSession.builder.appName("Spark SQL Example").getOrCreate()

  # Create a DataFrame
  data = [("James", "Smith", "USA", 30),
          ("Anna", "Rose", "UK", 25),
          ("Robert", "Williams", "USA", 45)]
  columns = ["firstname", "lastname", "country", "age"]

  df = spark.createDataFrame(data, columns)
  df.createOrReplaceTempView("people")

  # Run SQL queries
  sqlDF = spark.sql("SELECT * FROM people WHERE age > 30")
  sqlDF.show()
  spark.stop()
****************************************************************
Step 2 Run the script:
****************************************************************
  spark-submit spark_sql_example.py
****************************************************************
Apache Spark Tools and Commands:
****************************************************************
1. spark-submit
****************************************************************
   Purpose Used to submit Spark applications to a cluster for execution.
****************************************************************   
   Syntax
****************************************************************
     spark-submit \
       --class <main-class> \
       --master <master-url> \
       --deploy-mode <deploy-mode> \
       --executor-memory <memory-per-executor> \
       --total-executor-cores <num-cores> \
       <application-jar> [application-arguments]
****************************************************************
   Example
****************************************************************
     spark-submit \
       --class com.example.MySparkApp \
       --master spark://localhost:7077 \
       --deploy-mode client \
       --executor-memory 2g \
       --total-executor-cores 4 \
****************************************************************
2. Spark on Terminal: 
****************************************************************
(spark-shell for Scala, pyspark for Python)
****************************************************************
   Purpose Interactive shell for running Spark code snippets and testing. The Spark Shell is an interactive environment provided by Apache Spark that allows you to quickly prototype, test, and execute Spark commands in a REPL (Read-Eval-Print Loop) environment. It is especially useful for data exploration and iterative development, as it provides immediate feedback on your code.
****************************************************************   
   Usage
     - Scala: `spark-shell`
     - Python: `pyspark`
****************************************************************
To start the PySpark shell, open your terminal and type:
	pyspark
****************************************************************
This command initializes the PySpark environment and opens an interactive shell where you can start writing Spark commands in Python.
****************************************************************
it automatically creates a SparkSession and a SparkContext for you:
****************************************************************
Try the following Python code
****************************************************************
from pyspark.sql import SparkSession

# SparkSession automatically created as 'spark'
print(spark)

# SparkContext available as 'sc'
print(sc)
****************************************************************
Creating RDDs:
****************************************************************
You can create Resilient Distributed Datasets (RDDs) from a Python collection or an external data source. use this code:
****************************************************************

# Create an RDD from a Python list
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Perform a transformation (map) and action (collect)
squares = rdd.map(lambda x: x * x)
print(squares.collect())
****************************************************************
Working with DataFrames:
****************************************************************
DataFrames provide a higher-level abstraction over RDDs and are optimized for performance. use this code:
****************************************************************
# Creating a DataFrame from a JSON file
df = spark.read.json("path/to/json/file")

# Show the first few rows of the DataFrame
df.show()

# Print the schema of the DataFrame
df.printSchema()
****************************************************************
Running SQL Queries:
****************************************************************
You can run SQL queries directly on DataFrames by registering them as temporary views. use this code:
****************************************************************

# Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView("table")

# Running a SQL query
result = spark.sql("SELECT * FROM table WHERE age > 21")
result.show()


****************************************************************
3. Spark SQL (spark-sql for SQL queries)
****************************************************************
   Purpose Executes SQL queries against Spark data sources.
   Usage `spark-sql`
   Example
****************************************************************
     SELECT * FROM table WHERE column = 'value';
****************************************************************
4. Spark Streaming (spark-submit for streaming jobs)
****************************************************************
   Purpose Real-time processing of streaming data.
   Usage `spark-submit` with streaming-specific configurations.
   Example
****************************************************************
     spark-submit \
       --class com.example.StreamingApp \
       --master yarn \
       --deploy-mode cluster \
       --executor-memory 2g \
       --total-executor-cores 4 \
       my-streaming-app.jar arg1 arg2

****************************************************************
5. Spark MLlib (machine learning library)
****************************************************************
   Purpose Provides machine learning algorithms and utilities.
   Usage Import MLlib classes in Spark applications for machine learning tasks.
   Example (Scala)
     ```scala
     import org.apache.spark.ml.classification.LogisticRegression
     val lr = new LogisticRegression()
     ```
****************************************************************
6. Spark Web UI
****************************************************************
   Purpose Web-based interface for monitoring Spark applications and cluster resources.
   Access Typically available at `http://<driver-node>:4040`.
****************************************************************
7. Spark History Server
****************************************************************
   Purpose Retains information about completed Spark applications and allows viewing historical data and logs.
   Access Typically available at `http://<history-server>:18080`.
****************************************************************
Cluster Managers 
****************************************************************
Spark can run on various cluster managers like Spark's standalone cluster manager, Apache Hadoop YARN, or Apache Mesos. The `--master` parameter in `spark-submit` specifies the cluster manager.
****************************************************************
Resource Allocation Use `--executor-memory` and `--total-executor-cores` in `spark-submit` to allocate resources to Spark executors.
****************************************************************
Environment Setup Set `SPARK_HOME`, update `PATH`, and configure other environment variables for seamless Spark usage.
****************************************************************
Integration Spark integrates with other big data tools and ecosystems, such as Apache Hadoop, Apache Hive, Apache HBase, etc.
****************************************************************
Understanding and using these tools and commands efficiently will help in developing, deploying, and managing Apache Spark applications effectively in various use cases and environments.

****************************************************************
Using Spark on AWS:
****************************************************************
For large-scale applications, you might want to run Spark on a cluster. AWS provides several ways to run Spark:
****************************************************************
Amazon EMR (Elastic MapReduce) Managed cluster platform that simplifies running big data frameworks such as Apache Spark.
****************************************************************
  aws emr create-cluster --name "Spark cluster" \
    --release-label emr-6.2.0 \
    --applications Name=Spark \
    --ec2-attributes KeyName=myKey \
    --instance-type m5.xlarge \
    --instance-count 3
****************************************************************
Amazon EKS (Elastic Kubernetes Service) You can run Spark on Kubernetes clusters.
AWS Glue Fully managed ETL service that can run Spark jobs.
****************************************************************
Apache Spark and Apache Hadoop
****************************************************************
While Spark originally started as a project in the Hadoop ecosystem and can integrate well with Hadoop's distributed file system (HDFS) and resource manager (YARN), it does not have a strict dependency on Hadoop. Here are a few points to consider:
****************************************************************
    Stand-alone Mode: Spark can be run in stand-alone mode, where it uses its own built-in cluster manager. In this mode, Spark does not require Hadoop's components such as HDFS or YARN. It manages its own resources and scheduling.
****************************************************************
    Local Mode: For development and testing purposes, Spark can also run in local mode on a single machine without the need for a Hadoop cluster. This is particularly useful for getting started with Spark, experimenting with code, and running small-scale jobs.
****************************************************************
    Compatibility with Other Storage Systems: While Spark can work seamlessly with Hadoop's HDFS, it's also compatible with other storage systems such as Amazon S3, Azure Blob Storage, Google Cloud Storage, and various databases (e.g., Apache Cassandra, Apache HBase) through connectors and libraries.
****************************************************************
    Resource Managers: Spark supports multiple cluster managers apart from Hadoop's YARN, such as Apache Mesos and its stand-alone cluster manager. This flexibility allows users to choose the cluster management system that best fits their needs and infrastructure.
****************************************************************
    Dependencies: When you download and install Apache Spark, you'll notice that it includes all the necessary libraries and components to run Spark applications independently. This includes Spark Core, Spark SQL, MLlib, Spark Streaming, and GraphX, among others.
****************************************************************
So, if your use case doesn't require Hadoop-specific functionalities or if you're just getting started with Spark, you can install and use Spark without setting up Hadoop. However, if you're dealing with large-scale distributed data storage (e.g., HDFS) or need integration with Hadoop's ecosystem components, then using Spark with Hadoop would be appropriate.
****************************************************************
